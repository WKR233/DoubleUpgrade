## 徐奕辰、王柯然
### 算法设计
#### PPO算法
在模型训练上，我们是通过的PPO算法来进行的强化学习。PPO，即Proximal Policy Optimization，近端策略优化，这是一个强化学习中常用的算法，在性能上表现优异，同时容易调整。

PPO算法具体的步骤是基于对策略梯度方法的改进，主要包括：
- 收集数据：即通过在环境中执行策略来收集交互数据，包括状态、动作与奖励等。在代码中体现为actor利用已有的模型进行对战，收集reward的过程。
- 估计优势：使用优势估计算法来计算$\hat{A}_t^{\pi_k}$，可以使用时间差分或者广义优势估计等方法。
- 优化PPO loss：一般通过优化某一个特殊设计的目标函数来确定之后更新策略的方向
$$
L(\theta) = E(\min(r_t(\theta)\hat{A}, clip(r_t(\theta), 1-\epsilon,1+\epsilon)\hat{A}))
$$
这里的$r_t(\theta)$是指的重要性采样，$\hat{A}$是对优势函数的估计，$\epsilon$是一个小的正数，$clip()$函数则是对$r_t(\theta)$的限制，以保证更新的步骤不会太大。
- 更新策略：使用前一步得到的损失函数$L(\theta)$，我们可以利用梯度上升法来更新策略参数$\theta$
$$
\theta\leftarrow\theta+\alpha\nabla_\theta L(\theta)
$$
并重复以上步骤。在代码中体现为在`learner.py`中计算PPO loss并更新模型，`model_pool.py`把新的模型推入模型池`\model-pool`，`actor.py`从`\model-pool`中取用最新的模型并重新收集数据。

此外，我们还采用了一些技巧来加强我们对数据的利用，例如在`replay_buffer.py`中，我们使用经验回放池来：
	（1）保证交互序列内部的相关性对模型训练不会产生较大的影响
	（2）提高对样本使用的效率
通过对经验回放池的使用，我们能够提升样本的使用率，可以区分样本的重要程度，同时尽量消除样本选取产生的有偏性。

#### 网络搭建
对于网络模型的搭建，经过多次调整与测试之后，我们最终采用了六层二维卷积层，并把他们的维度设置如下：
```
nn.Conv2d(128, 256, 3, 1, 1, bias = False),
nn.Conv2d(256, 512, 3, 1, 1, bias = False),
nn.Conv2d(512, 256, 3, 1, 1, bias = False),
nn.Conv2d(256, 128, 3, 1, 1, bias = False),
nn.Conv2d(128, 64, 3, 1, 1, bias = False),
nn.Conv2d(64, 32, 3, 1, 1, bias = False),
```
在每两个相邻的卷积层中，我们都加入了`ReLU`层以保证网络的有效性。因此，我们可以得到网络的中间隐藏状态`hidden`。
在此之后，我们将数据展平为一维，并让`hidden`经过一个由两层全连接层与一层ReLU层构成的网络：
```
nn.Linear(32 * 4 * 14, 256),
nn.ReLU(True),
nn.Linear(256, 54)
```
经过这个网络之后，我们就成功得到了一个对54张牌出现概率的预测，模型就可以根据最大的概率来选择它的出牌策略。
而对于这一策略的价值评估，我们采用了另一个由两层全连接层与一层ReLU层构成的网络：
```
nn.Linear(32 * 4 * 14, 256),
nn.ReLU(True),
nn.Linear(256, 1)
```
`hidden`经过这个网络后会变为一个单独的数值`value`，我们可以利用它来优化后续的网络。
#### 盖牌策略
#### reward调整
### 实验结果

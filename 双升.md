# 双升多智能体强化学习报告
## 算法设计
在双升游戏中，游戏共有三个阶段：发牌阶段，盖牌阶段和出牌阶段。在发牌阶段，玩家在拿到牌后需要决定是否叫主/反主，以确定主牌的花色和第一轮游戏确定庄家。而庄家拥有盖牌阶段将底牌八张牌与手牌互换。因此，三个阶段实际上有着不同的价值评估和各自的策略。在这里我们整个游戏中的算法将Rule-based和Learning-based相结合，由于前两个阶段决策较为简单，我们采取Rule-based的方法使得我们的智能体手牌尽量占到优势，出牌阶段用强化学习的方法使智能体自己学习怎么优化自己的策略。
## Rule-based 算法设计
### 发牌阶段
在发牌阶段，我们每回合都对摸到的牌和手牌进行遍历。基于人类玩双升游戏的经验，由于我们不希望自己最终手里的主牌太少，所以只有当摸到牌的花色共有五张以上，并且我们有对应花色的级牌才会叫主。基于同样的理念，如果反主，我们也不希望主牌太短，所以只有该花色牌在五张以上时，遍历发现自己有两张同花色级牌后反主。无主情况比较少见，而且打牌的难度也更大在此不采取反无主的策略。
### 盖牌阶段
在盖牌的时候，我们希望把底牌中的主牌都拿到手里，而且只取得底牌中较大的副牌，或者能够和手牌构成对子的牌。为了防止抠底被对方拿到太多分，我们将底牌中的5都重新盖回，随后遍历手牌将小牌逐一加入弃牌堆直至达到8张。
## 强化学习算法设计
### PPO算法
在模型训练上，我们是通过的PPO算法来进行的强化学习。PPO，即Proximal Policy Optimization，近端策略优化，这是一个强化学习中常用的算法，在性能上表现优异，同时容易调整。

PPO算法具体的步骤是基于对策略梯度方法的改进，主要包括：
- 收集数据：即通过在环境中执行策略来收集交互数据，包括状态、动作与奖励等。在代码中体现为actor利用已有的模型进行对战，收集reward的过程。
- 估计优势：使用优势估计算法来计算$\hat{A}_t^{\pi_k}$，可以使用时间差分或者广义优势估计等方法。
- 优化PPO loss：一般通过优化某一个特殊设计的目标函数来确定之后更新策略的方向
$$
L(\theta) = E(\min(r_t(\theta)\hat{A}, clip(r_t(\theta), 1-\epsilon,1+\epsilon)\hat{A}))
$$
这里的$r_t(\theta)$是指的重要性采样，$\hat{A}$是对优势函数的估计，$\epsilon$是一个小的正数，$clip()$函数则是对$r_t(\theta)$的限制，以保证更新的步骤不会太大。
- 更新策略：使用前一步得到的损失函数$L(\theta)$，我们可以利用梯度上升法来更新策略参数$\theta$
$$
\theta\leftarrow\theta+\alpha\nabla_\theta L(\theta)
$$
并重复以上步骤。在代码中体现为在`learner.py`中计算PPO loss并更新模型，`model_pool.py`把新的模型推入模型池`\model-pool`，`actor.py`从`\model-pool`中取用最新的模型并重新收集数据。

此外，我们还采用了一些技巧来加强我们对数据的利用，例如在`replay_buffer.py`中，我们使用经验回放池来：
	（1）保证交互序列内部的相关性对模型训练不会产生较大的影响
	（2）提高对样本使用的效率
通过对经验回放池的使用，我们能够提升样本的使用率，可以区分样本的重要程度，同时尽量消除样本选取产生的有偏性。

### 网络搭建
对于网络模型的搭建，经过多次调整与测试之后，我们最终采用了六层二维卷积层，并把他们的维度设置如下：
```
nn.Conv2d(128, 256, 3, 1, 1, bias = False),
nn.Conv2d(256, 512, 3, 1, 1, bias = False),
nn.Conv2d(512, 256, 3, 1, 1, bias = False),
nn.Conv2d(256, 128, 3, 1, 1, bias = False),
nn.Conv2d(128, 64, 3, 1, 1, bias = False),
nn.Conv2d(64, 32, 3, 1, 1, bias = False),
```
在每两个相邻的卷积层中，我们都加入了`ReLU`层以保证网络的有效性。因此，我们可以得到网络的中间隐藏状态`hidden`。
在此之后，我们将数据展平为一维，并让`hidden`经过一个由两层全连接层与一层ReLU层构成的网络：
```
nn.Linear(32 * 4 * 14, 256),
nn.ReLU(True),
nn.Linear(256, 54)
```
经过这个网络之后，我们就成功得到了一个对54张牌出现概率的预测，模型就可以根据最大的概率来选择它的出牌策略。
而对于这一策略的价值评估，我们采用了另一个由两层全连接层与一层ReLU层构成的网络：
```
nn.Linear(32 * 4 * 14, 256),
nn.ReLU(True),
nn.Linear(256, 1)
```
`hidden`经过这个网络后会变为一个单独的数值`value`，我们可以利用它来优化后续的网络。
### Reward调整
我们希望在出牌阶段，智能体能够学习到一些人类打牌时的经验，所以对每一轮反馈的Reward进行了改进。每次分牌的对Reward影响肯定是最首要的，所以我们选择将改进部分的Reward都控制在5分以内。我们添加的Reward可以让智能体从每一轮出牌中都获得经验，而不是只有得分情况下才能够更新策略。
具体的设计如下：
1. 如果一轮中胜方是自己的对家，出分牌 +2分
2. 如果一轮中胜方是自己的邻家，出小牌 +1分，出分牌 -2分
3. 如果一轮中胜方是自己，用主毙 +1分
4. 如果一轮中第一个出牌的人，出副牌A +1分，对子 +2分，甩牌 +3分，拖拉机 +3分

我们希望通过这个Reward让智能体不要在对方牌大的时候送分，在自家牌大的时候才给分。并且想让他学会在自己一门副牌打完后用主毙牌获得主动权，而不是贴牌。而且如果我们占领了先机，就要用副牌中的大牌帮助同伴跑分，或者用对子、甩牌、拖拉机占据上风。这也是跟人类打牌时的策略相似。
## 实验结果
